{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5fc0db-8671-45eb-9006-8236cda57d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,\"/home/showalte/research/prob_seq_queries/\")\n",
    "from seq_queries.utils import read_pkl, write_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87bf22-bcf6-49ea-b43c-9c6bd34216f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimate fields\n",
    "- ground_truth/beam_search_lb\n",
    "- Importance sampling estimate\n",
    "- Hybrid estimate\n",
    "- Importance sampling variance\n",
    "- Hybrid variance\n",
    "\n",
    "# General Metadata\n",
    "- dataset_name\n",
    "- experiment_name\n",
    "- history_id\n",
    "- Excluded term\n",
    "- sequence_length\n",
    "- history_length\n",
    "- total_sequence_length\n",
    "\n",
    "# Sampling metadata\n",
    "- num_mc_samples (sub_estimates)\n",
    "- sample_model_iters\n",
    "\n",
    "# Hybrid data\n",
    "- hybrid_model_iters\n",
    "\n",
    "# Beam search metadata\n",
    "- min_variance\n",
    "- search_model_iters\n",
    "- min_variance_reduction\n",
    "- true_coverage\n",
    "- restricted_coverage\n",
    "- num_beams\n",
    "- top_k\n",
    "- top_p\n",
    "- (beam search) interpolation_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b52721a-1c0f-4001-b451-3899e8e6176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_data(experiment, dataset, h, s, root=\"../data\", \n",
    "            methods=['beam_search_is_hybrid','importance_sampling','random_sampling','beam_search'],\n",
    "            gt_methods=['ground_truth','beam_search']):\n",
    "    \n",
    "        data_dict = {}\n",
    "        gt_type=None\n",
    "        for method in methods:\n",
    "            template_path=root + f\"/{method}/{dataset}/{experiment}/\"\n",
    "            template_file=(f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_\" +\n",
    "            f\"{method.replace('_','-')}_{h}h_{s}s*mc.pkl\")\n",
    "            pot_pattern = os.path.join(template_path,template_file)\n",
    "            pot_paths = glob.glob(pot_pattern)\n",
    "            assert len(pot_paths) == 1,\\\n",
    "                f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "            print(method, pot_paths[0])\n",
    "            data_dict[method]= read_pkl(pot_paths[0])\n",
    "            data_dict[method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "        for gt_method in gt_methods:\n",
    "            try:\n",
    "                template_path=root + f\"/{gt_method}/{dataset}/{experiment}/\"\n",
    "                # Don't match on model budget terms\n",
    "                template_file=(f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_\" +\n",
    "                               f\"{gt_method.replace('_','-')}_{h}h_{s}s*[!t].pkl\")\n",
    "                pot_pattern = os.path.join(template_path,template_file)\n",
    "                pot_paths = glob.glob(pot_pattern)\n",
    "                assert len(pot_paths) == 1,\\\n",
    "                    f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "                print(\"GT: \", gt_method, \"\\n\",pot_paths[0],\"\\n=============\")\n",
    "                data_dict[gt_method]= read_pkl(pot_paths[0])\n",
    "                data_dict[gt_method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "                data_dict['gt_type'] = gt_method\n",
    "                return data_dict\n",
    "            except: pass\n",
    "        assert False,\"Could not find ground truth\"\n",
    "        print()\n",
    "        return None\n",
    "    \n",
    "def get_experiment_data_model_budget(experiment, dataset, h, s, root=\"../data\", \n",
    "            methods=['beam_search_is_hybrid','importance_sampling','random_sampling','beam_search'],\n",
    "            gt_methods=['ground_truth','beam_search']):\n",
    "    \n",
    "        data_dict = {}\n",
    "        gt_type=None\n",
    "        for method in methods:\n",
    "            template_path=root + f\"/{method}/{dataset}/{experiment}/\"\n",
    "            template_file=(f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_\"\n",
    "            + f\"{method.replace('_','-')}_{h}h_{s}s*{'' if method == 'beam_search_is_hybrid' else 'model-budget'}.pkl\")\n",
    "            pot_pattern = os.path.join(template_path,template_file)\n",
    "            pot_paths = glob.glob(pot_pattern)\n",
    "            assert len(pot_paths) == 1,\\\n",
    "                f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "            # print(method,pot_paths[0])\n",
    "            print(method, \"\\n\",pot_paths[0],\"\\n=============\")\n",
    "            data_dict[method]= read_pkl(pot_paths[0])\n",
    "            data_dict[method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "        for gt_method in gt_methods:\n",
    "            try:\n",
    "                template_path=root + f\"/{gt_method}/{dataset}/{experiment}/\"\n",
    "                template_file=(f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_\" +\n",
    "                               f\"{gt_method.replace('_','-')}_{h}h_{s}s*.pkl\")\n",
    "                pot_pattern = os.path.join(template_path,template_file)\n",
    "                pot_paths = glob.glob(pot_pattern)\n",
    "                assert len(pot_paths) == 1,\\\n",
    "                    f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "                print(\"GT: \", gt_method, \"\\n\",pot_paths[0],\"\\n=============\")\n",
    "                data_dict[gt_method]= read_pkl(pot_paths[0])\n",
    "                data_dict[gt_method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "                data_dict['gt_type'] = gt_method\n",
    "                return data_dict\n",
    "            except: pass\n",
    "        assert False,\"Could not find ground truth\"\n",
    "        return None\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bff8c61a-43ae-4901-8cf0-1f34078701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sampling(samp_dict,sub_estimates,sample_type=\"importance\"):\n",
    "    samp_estimates = samp_dict['sample_estimates'][:,:len(sub_estimates)]\n",
    "    if not sub_estimates:\n",
    "        if len(samp_estimates.shape) ==3:\n",
    "            assert samp_estimates.shape[1] == samp_dict['metadata']['num_mc_samples'],\\\n",
    "            (f\"Error, estimate dimensions are {samp_estimates.shape} but the number of samples is \" +\n",
    "             f\"{samp_dict['metadata']['num_mc_samples']}, which does not match\")\n",
    "            \n",
    "            samp_estimates = torch.gather(samp_estimates.mean(dim=1),1,\n",
    "                                          samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "            assert len(samp_estimates.shape) == 1,f\"Shape of imp_samp_estimates is {len(samp_estimates.shape)}\"\n",
    "        if len(samp_estimates.shape) ==2:\n",
    "            samp_estimates = torch.gather(samp_estimates,1,\n",
    "                                              samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "        \n",
    "        df = pd.DataFrame(samp_estimates)\n",
    "        df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "        df[f'{sample_type}_model_iters'] = samp_dict['model_iters']\n",
    "        df[f'{sample_type}_est_variance'] = samp_dist['sample_estimate_var']\n",
    "        \n",
    "    else:\n",
    "        assert samp_estimates.shape[1] == len(sub_estimates),\\\n",
    "        (\"Importance sampling estimates and sub_estimates are not aligned in shape.\" +\n",
    "         f\"got sample_est: {samp_estimates.shape[1]} and sub_estimates: {len(sub_estimates)}\")\n",
    "        if len(samp_estimates.shape) == 2:\n",
    "            samp_estimates = pd.DataFrame(samp_estimates,columns=sub_estimates)\n",
    "            samp_var = pd.DataFrame(samp_dict['sample_estimate_var'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            model_iters_df = pd.DataFrame(samp_dict['model_iters'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            df = pd.melt(samp_estimates,value_vars=sub_estimates)\n",
    "            df.columns = ['num_samples','sample_estimate']\n",
    "            var_df =  pd.melt(samp_var,value_vars=sub_estimates)\n",
    "            var_df.columns = ['num_samples','variance']\n",
    "            iter_df = pd.melt(model_iters_df,value_vars=sub_estimates)\n",
    "            iter_df.columns = ['num_samples','model_iters']\n",
    "            df[f'{sample_type}_model_iters']=iter_df['model_iters']\n",
    "            df[f'{sample_type}_est_variance']=var_df['variance']\n",
    "            \n",
    "        elif len(samp_estimates.shape) == 3:\n",
    "            df_list = []\n",
    "            for i in range(len(sub_estimates)):\n",
    "                df = pd.DataFrame(\n",
    "                            torch.gather(samp_estimates[:,i],1,\n",
    "                                      samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                        )\n",
    "                df.insert(0,'num_mc_samples',sub_estimates[i])\n",
    "                df[f'{sample_type}_model_iters'] = samp_dict['model_iters'][:,i]\n",
    "                print(samp_dict['sample_estimate_var'].shape)\n",
    "                samp_est_var = torch.gather(samp_dict['sample_estimate_var'][:,i],1,\n",
    "                                            samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                                            \n",
    "                df[f'{sample_type}_est_variance'] = samp_est_var\n",
    "                df_list.append(df)\n",
    "            df = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "        else:\n",
    "            assert False,f\"Shape of samp_estimates is {len(samp_estimates.shape)}\"\n",
    "    assert df.shape[-1] == 4, f\"DF shape is {df.shape}\"\n",
    "    df.columns = ['num_mc_samples',f'{sample_type}_sampling_est',\n",
    "                  f'{sample_type}_model_iters',f'{sample_type}_est_variance']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def flatten_gt(data_dict,gt_type):\n",
    "    gt_dict = data_dict[gt_type]\n",
    "    # gt = gt_dict['bs_lower_bound']\n",
    "    gt = torch.gather(gt_dict['bs_lower_bound'],1,\n",
    "                      gt_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "    assert len(gt.shape) == 1,\\\n",
    "    f\"Ground truth has {len(gt.shape)} dimensions\"\n",
    "    df = pd.DataFrame(gt,columns=['ground_truth'])\n",
    "    for item in ['true_coverage','restricted_coverage']:\n",
    "        df[item] = [gti.item() for gti in gt_dict[item]]\n",
    "    # df[\"gt_model_iters\"] = gt_dict['model_iters']\n",
    "    df['gt_type'] = gt_type\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def flatten_experiment(data_dict,experiment, dataset,h,s,\n",
    "     global_agreement_vals= ['excluded_terms']):\n",
    "    sub_estimates = sorted(list(\n",
    "        # set(data_dict['importance_sampling']['metadata']['sub_estimates']) &\n",
    "        # set(data_dict['random_sampling']['metadata']['sub_estimates']) &\n",
    "        set(data_dict['beam_search_is_hybrid']['metadata']['sub_estimates'])))\n",
    "    sub_est_len = 1 if not sub_estimates else len(sub_estimates)\n",
    "    importance_df = flatten_sampling(data_dict['importance_sampling'],sub_estimates,sample_type ='importance')\n",
    "    random_df = flatten_sampling(data_dict['random_sampling'],sub_estimates,sample_type ='random')\n",
    "    hybrid_df = flatten_sampling(data_dict['beam_search_is_hybrid'],sub_estimates,sample_type ='hybrid')\n",
    "    hybrid_df.drop(\"num_mc_samples\",inplace = True,axis=1)\n",
    "    \n",
    "    gt_df = flatten_gt(data_dict,data_dict['gt_type'])\n",
    "    gt_df = pd.concat([gt_df]*sub_est_len,axis=0,ignore_index=True)\n",
    "    final_df = pd.concat([importance_df,hybrid_df,gt_df],axis=1)\n",
    "    \n",
    "    # Metadata checks\n",
    "    is_metadata = ['top_k','top_p']\n",
    "    bs_metadata = ['min_variance','min_var_reduction','num_beams']\n",
    "    for m in is_metadata:\n",
    "        final_df[m] = data_dict['importance_sampling']['metadata'][m]\n",
    "    for m in bs_metadata:\n",
    "        final_df[m] = data_dict[data_dict['gt_type']]['metadata'][m]\n",
    "    final_df['interp_func'] = str(data_dict[data_dict['gt_type']]['metadata']['interp_func']).split(\" \")[1]\n",
    "    \n",
    "    \n",
    "    final_df['dataset_name'] = dataset\n",
    "    final_df['hist_len'] = h\n",
    "    final_df['total_seq_len'] = s\n",
    "    final_df['seq_len'] = s-h\n",
    "    sequence_ids = list(range(data_dict['importance_sampling']['sample_estimates'].shape[0]))*sub_est_len\n",
    "    excluded_terms = data_dict['importance_sampling']['excluded_terms'].numpy().tolist()*sub_est_len\n",
    "    final_df['sequence_id'] = sequence_ids\n",
    "    final_df['excluded_term'] = excluded_terms\n",
    "    phase_shifts = read_pkl(\"/srv/disk00/samshow/amazon/amazon_phase_trans.pkl\")\n",
    "    phase_shift_val_inds = read_pkl(\"../data/amazon/amazon_val_dl_transition_inds.pkl\")\n",
    "    phase_shifts = phase_shifts[phase_shift_val_inds].numpy().tolist()\n",
    "    # print(phase_shift_val_inds.shape, final_df.shape)\n",
    "    # final_df['phase_shifts'] = phase_shifts * sub_est_len\n",
    "    # final_df['phase_shifts'] -=1\n",
    "    \n",
    "    return final_df\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb936c89-eb0e-42cd-923b-8a60a9fc167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = [\"val_dl\"]\n",
    "dataset = [\"apps\"]\n",
    "lengths = [(13,15),(12,15)]\n",
    "def flatten_experiments(experiments, datasets, lengths,model_budget=False):\n",
    "    data_list = []\n",
    "    for experiment in experiments:\n",
    "        for dataset in datasets:\n",
    "            for h,s in lengths:\n",
    "                # try:\n",
    "                    \n",
    "                    if model_budget:\n",
    "                        data = get_experiment_data(experiment,dataset,h,s)\n",
    "                    else:\n",
    "                        data = get_experiment_data_model_budget(experiment,dataset,h,s)\n",
    "                        \n",
    "                    df = flatten_experiment(data,experiment, dataset, h,s)\n",
    "                    \n",
    "                    # print(df.head())\n",
    "                    # print(df.columns)\n",
    "                    # print(df.shape)\n",
    "                    # sys.exit(1)\n",
    "                    \n",
    "                    data_list.append(df)\n",
    "                    \n",
    "    # print(len(data_list))\n",
    "    data_df = pd.concat(data_list,axis = 0)\n",
    "    ordering = [ 'dataset_name','sequence_id','seq_len', 'excluded_term', 'gt_type','ground_truth','importance_sampling_est','hybrid_sampling_est', \n",
    "                'num_mc_samples', 'importance_model_iters','hybrid_model_iters',\n",
    "       'importance_est_variance',  'hybrid_est_variance',\n",
    "         'true_coverage', 'restricted_coverage',  'top_k', 'top_p', 'min_variance',\n",
    "       'min_var_reduction', 'num_beams', 'interp_func',\n",
    "       'hist_len', 'total_seq_len']#, 'phase_shifts']\n",
    "    data_df = data_df[ordering]\n",
    "    print(data_df.columns)\n",
    "    \n",
    "    return data_df\n",
    "                \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62c51bbf-1517-42cc-8ed2-b99b496d041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_search_is_hybrid \n",
      " ../data/beam_search_is_hybrid/shakespeare/val_dl/val-dl_shakespeare_beam-search-is-hybrid_18h_20s_1000mc.pkl \n",
      "=============\n",
      "importance_sampling \n",
      " ../data/importance_sampling/shakespeare/val_dl/val-dl_shakespeare_importance-sampling_18h_20s_1000mc_model-budget.pkl \n",
      "=============\n",
      "random_sampling \n",
      " ../data/random_sampling/shakespeare/val_dl/val-dl_shakespeare_random-sampling_18h_20s_1000mc_model-budget.pkl \n",
      "=============\n",
      "beam_search \n",
      " ../data/beam_search/shakespeare/val_dl/val-dl_shakespeare_beam-search_18h_20s_model-budget.pkl \n",
      "=============\n",
      "GT:  ground_truth \n",
      " ../data/ground_truth/shakespeare/val_dl/val-dl_shakespeare_ground-truth_18h_20s.pkl \n",
      "=============\n",
      "torch.Size([6861, 68])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f3a26aee5da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_dl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shakespeare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df = flatten_experiments(['val_dl'],['shakespeare'],[(18,20)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-940492c0fe6b>\u001b[0m in \u001b[0;36mflatten_experiments\u001b[0;34m(experiments, datasets, lengths, model_budget)\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_experiment_data_model_budget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;31m# print(df.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-ad438e33210a>\u001b[0m in \u001b[0;36mflatten_experiment\u001b[0;34m(data_dict, experiment, dataset, h, s, global_agreement_vals)\u001b[0m\n\u001b[1;32m     85\u001b[0m         set(data_dict['beam_search_is_hybrid']['metadata']['sub_estimates'])))\n\u001b[1;32m     86\u001b[0m     \u001b[0msub_est_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msub_estimates\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mimportance_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'importance_sampling'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_estimates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_type\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mrandom_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_sampling'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_estimates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_type\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mhybrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beam_search_is_hybrid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_estimates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_type\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'hybrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-ad438e33210a>\u001b[0m in \u001b[0;36mflatten_sampling\u001b[0;34m(samp_dict, sub_estimates, sample_type)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{sample_type}_model_iters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_iters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_estimate_var'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 samp_est_var = torch.gather(samp_dict['sample_estimate_var'][:,i],1,\n\u001b[0m\u001b[1;32m     49\u001b[0m                                             samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "df = flatten_experiments(['val_dl'],['shakespeare'],[(18,20),(17,20)])\n",
    "# df = flatten_experiments(['val_dl'],['shakespeare'],[(18,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ce843b1-19fd-4a08-bd5e-a76576456916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance_model_iters</th>\n",
       "      <th>hybrid_model_iters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6811</th>\n",
       "      <td>3000</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>3000</td>\n",
       "      <td>2936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6813</th>\n",
       "      <td>3000</td>\n",
       "      <td>2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>3000</td>\n",
       "      <td>2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>3000</td>\n",
       "      <td>2290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>3000</td>\n",
       "      <td>2638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>3000</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>3000</td>\n",
       "      <td>2661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>3000</td>\n",
       "      <td>2553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>3000</td>\n",
       "      <td>2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>3000</td>\n",
       "      <td>2107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6822</th>\n",
       "      <td>3000</td>\n",
       "      <td>2267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6823</th>\n",
       "      <td>3000</td>\n",
       "      <td>2865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>3000</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6825</th>\n",
       "      <td>3000</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>3000</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827</th>\n",
       "      <td>3000</td>\n",
       "      <td>2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6828</th>\n",
       "      <td>3000</td>\n",
       "      <td>2925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6829</th>\n",
       "      <td>3000</td>\n",
       "      <td>1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>3000</td>\n",
       "      <td>1890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6831</th>\n",
       "      <td>3000</td>\n",
       "      <td>2465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>3000</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>3000</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>3000</td>\n",
       "      <td>2802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>3000</td>\n",
       "      <td>2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>3000</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>3000</td>\n",
       "      <td>2419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>3000</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>3000</td>\n",
       "      <td>2032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>3000</td>\n",
       "      <td>2430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841</th>\n",
       "      <td>3000</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6842</th>\n",
       "      <td>3000</td>\n",
       "      <td>2670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6843</th>\n",
       "      <td>3000</td>\n",
       "      <td>2135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6844</th>\n",
       "      <td>3000</td>\n",
       "      <td>2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>3000</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>3000</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6847</th>\n",
       "      <td>3000</td>\n",
       "      <td>2352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>3000</td>\n",
       "      <td>2793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>3000</td>\n",
       "      <td>2117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>3000</td>\n",
       "      <td>2899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>3000</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>3000</td>\n",
       "      <td>2418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>3000</td>\n",
       "      <td>1568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>3000</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>3000</td>\n",
       "      <td>2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>3000</td>\n",
       "      <td>2752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>3000</td>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>3000</td>\n",
       "      <td>2794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>3000</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>3000</td>\n",
       "      <td>2690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      importance_model_iters  hybrid_model_iters\n",
       "6811                    3000                2304\n",
       "6812                    3000                2936\n",
       "6813                    3000                2803\n",
       "6814                    3000                2923\n",
       "6815                    3000                2290\n",
       "6816                    3000                2638\n",
       "6817                    3000                2002\n",
       "6818                    3000                2661\n",
       "6819                    3000                2553\n",
       "6820                    3000                2856\n",
       "6821                    3000                2107\n",
       "6822                    3000                2267\n",
       "6823                    3000                2865\n",
       "6824                    3000                2550\n",
       "6825                    3000                1798\n",
       "6826                    3000                2525\n",
       "6827                    3000                2700\n",
       "6828                    3000                2925\n",
       "6829                    3000                1602\n",
       "6830                    3000                1890\n",
       "6831                    3000                2465\n",
       "6832                    3000                1569\n",
       "6833                    3000                2361\n",
       "6834                    3000                2802\n",
       "6835                    3000                2348\n",
       "6836                    3000                1098\n",
       "6837                    3000                2419\n",
       "6838                    3000                2094\n",
       "6839                    3000                2032\n",
       "6840                    3000                2430\n",
       "6841                    3000                1038\n",
       "6842                    3000                2670\n",
       "6843                    3000                2135\n",
       "6844                    3000                2409\n",
       "6845                    3000                1044\n",
       "6846                    3000                2029\n",
       "6847                    3000                2352\n",
       "6848                    3000                2793\n",
       "6849                    3000                2117\n",
       "6850                    3000                2899\n",
       "6851                    3000                1433\n",
       "6852                    3000                2418\n",
       "6853                    3000                1568\n",
       "6854                    3000                2555\n",
       "6855                    3000                2856\n",
       "6856                    3000                2752\n",
       "6857                    3000                2836\n",
       "6858                    3000                2794\n",
       "6859                    3000                1916\n",
       "6860                    3000                2690"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['importance_sampling']['metadata']['sub_estimates']\n",
    "# print(df.shape)\n",
    "# print(df.isnull().sum())\n",
    "# df.phase_shifts.describe()\n",
    "# df[['importance_model_iters','hybrid_model_iters']].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b17f57-5661-4ffc-8b42-d94827b114ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>excluded_term</th>\n",
       "      <th>gt_type</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>importance_sampling_est</th>\n",
       "      <th>hybrid_sampling_est</th>\n",
       "      <th>num_mc_samples</th>\n",
       "      <th>importance_model_iters</th>\n",
       "      <th>...</th>\n",
       "      <th>true_coverage</th>\n",
       "      <th>restricted_coverage</th>\n",
       "      <th>top_k</th>\n",
       "      <th>top_p</th>\n",
       "      <th>min_variance</th>\n",
       "      <th>min_var_reduction</th>\n",
       "      <th>num_beams</th>\n",
       "      <th>interp_func</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>total_seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.067603</td>\n",
       "      <td>0.046714</td>\n",
       "      <td>0.087690</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.201895</td>\n",
       "      <td>0.221774</td>\n",
       "      <td>0.177074</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.063420</td>\n",
       "      <td>0.063707</td>\n",
       "      <td>0.063389</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.866673</td>\n",
       "      <td>0.799308</td>\n",
       "      <td>0.868733</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.154992</td>\n",
       "      <td>0.296705</td>\n",
       "      <td>0.161840</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  sequence_id  seq_len  excluded_term       gt_type  \\\n",
       "0  shakespeare            0        2             42  ground_truth   \n",
       "1  shakespeare            1        2              1  ground_truth   \n",
       "2  shakespeare            2        2             32  ground_truth   \n",
       "3  shakespeare            3        2             20  ground_truth   \n",
       "4  shakespeare            4        2             46  ground_truth   \n",
       "\n",
       "   ground_truth  importance_sampling_est  hybrid_sampling_est  num_mc_samples  \\\n",
       "0      0.067603                 0.046714             0.087690              10   \n",
       "1      0.201895                 0.221774             0.177074              10   \n",
       "2      0.063420                 0.063707             0.063389              10   \n",
       "3      0.866673                 0.799308             0.868733              10   \n",
       "4      0.154992                 0.296705             0.161840              10   \n",
       "\n",
       "   importance_model_iters  ...  true_coverage  restricted_coverage  top_k  \\\n",
       "0                      20  ...       0.944602                  1.0      0   \n",
       "1                      20  ...       0.865448                  1.0      0   \n",
       "2                      20  ...       0.999394                  1.0      0   \n",
       "3                      20  ...       0.999972                  1.0      0   \n",
       "4                      20  ...       0.883579                  1.0      0   \n",
       "\n",
       "   top_p  min_variance  min_var_reduction  num_beams  interp_func  hist_len  \\\n",
       "0      0         False                0.9        1.0   lin_interp        18   \n",
       "1      0         False                0.9        1.0   lin_interp        18   \n",
       "2      0         False                0.9        1.0   lin_interp        18   \n",
       "3      0         False                0.9        1.0   lin_interp        18   \n",
       "4      0         False                0.9        1.0   lin_interp        18   \n",
       "\n",
       "   total_seq_len  \n",
       "0             20  \n",
       "1             20  \n",
       "2             20  \n",
       "3             20  \n",
       "4             20  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b010b1e-e070-4f9b-85bf-141fd935acab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'importance')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = df[df['phase_shifts'] > 10]\n",
    "plt.scatter(df['ground_truth'],df['importance_sampling_est'])\n",
    "plt.title(\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3e8a10-b1b5-4498-bc4a-90eaa5ecbba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance_est_variance</th>\n",
       "      <th>hybrid_est_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.191800e+04</td>\n",
       "      <td>2.191800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.688037e-05</td>\n",
       "      <td>3.907905e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.492985e-04</td>\n",
       "      <td>8.797111e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.860333e-20</td>\n",
       "      <td>1.116206e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.422974e-09</td>\n",
       "      <td>1.191007e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.011587e-08</td>\n",
       "      <td>1.700204e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.072154e-07</td>\n",
       "      <td>1.139661e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.445925e-02</td>\n",
       "      <td>4.038442e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       importance_est_variance  hybrid_est_variance\n",
       "count             2.191800e+04         2.191800e+04\n",
       "mean              3.688037e-05         3.907905e-06\n",
       "std               6.492985e-04         8.797111e-05\n",
       "min               2.860333e-20         1.116206e-19\n",
       "25%               4.422974e-09         1.191007e-09\n",
       "50%               4.011587e-08         1.700204e-08\n",
       "75%               3.072154e-07         1.139661e-07\n",
       "max               4.445925e-02         4.038442e-03"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['importance_est_variance','hybrid_est_variance']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ed16f5a-8501-46b6-8e3e-3d4f3a55b259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset_name               0\n",
       "sequence_id                0\n",
       "seq_len                    0\n",
       "excluded_term              0\n",
       "gt_type                    0\n",
       "ground_truth               0\n",
       "importance_sampling_est    0\n",
       "hybrid_sampling_est        0\n",
       "num_mc_samples             0\n",
       "importance_model_iters     0\n",
       "hybrid_model_iters         0\n",
       "importance_est_variance    0\n",
       "hybrid_est_variance        0\n",
       "true_coverage              0\n",
       "restricted_coverage        0\n",
       "top_k                      0\n",
       "top_p                      0\n",
       "min_variance               0\n",
       "min_var_reduction          0\n",
       "num_beams                  0\n",
       "interp_func                0\n",
       "hist_len                   0\n",
       "total_seq_len              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a52b7fd-4659-494d-9ff3-13584d46e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('shakespeare_17-18_20.csv',index=None)\n",
    "# df.to_csv('amazon_12-13_15_with-phase.csv',index=None)\n",
    "# df.to_csv('apps_12-13_15.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea7d4da0-609f-40e6-ad58-0c5d761c8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_dict = read_pkl(\"/srv/disk00/samshow/amazon/amazon_text_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb6f3eab-1f61-4c3e-bf7c-2ff76eb144c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = amazon_dict['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19594098-def2-47ac-8f3a-059d760179fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63844580, 16])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f0b0cc01-7d4b-4203-88ea-d367dd1d10cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 16., 16., 16., 11.,  3., 27., 27.,  9.,  9.,  9., 23., 16., 16.,\n",
       "        16.,  8.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94c73676-c082-46a2-997d-1cea95e17256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63844580/63844580 [17:23<00:00, 61164.82it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "trans = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    trans.append(torch.unique_consecutive(data[i,1:]).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b36271e-5d00-43d2-af28-de548ec74856",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torch.LongTensor(trans).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c83ae82d-e144-4f7b-9fa1-855591a0f8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63844580])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "36efdf43-a1a5-43b0-97c4-9f85633c5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pkl(trans, \"/srv/disask00/samshow/amazon/amazon_phase_trans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "60ad3b21-6ab1-4788-af46-5b60a4823228",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_vals = []\n",
    "for i in range(1,trans.max()+1):\n",
    "    trans_vals.append((trans == i).sum().item()/trans.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "299b9ef8-a587-479e-821d-6b244e68101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_perc_per_phase_shift = [0.059163910233257073,\n",
    " 0.020707521296247856,\n",
    " 0.056485656260876024,\n",
    " 0.04778584180520884,\n",
    " 0.07297364944682853,\n",
    " 0.07653857226408256,\n",
    " 0.09382401763783238,\n",
    " 0.10168632952084578,\n",
    " 0.1102257544806466,\n",
    " 0.10986890351538063,\n",
    " 0.09992870185691566,\n",
    " 0.0774158276238954,\n",
    " 0.047934671979986396,\n",
    " 0.020711280425057224,\n",
    " 0.004749361652939059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726ba0a-3b52-4c66-93bd-57bbea35dc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
